

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Calibration Part2 (Robot Kinematic) &mdash; Rosvita documentation  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Force Torque Data" href="Force_Torque_Data.html" />
    <link rel="prev" title="Calibration Part1 (Camera, Hand-Eye, Endeffector)" href="Calibration_Part1.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/rosvita-docs-logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Getting_Started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="Main_View.html">ROSVITA Main View</a></li>
<li class="toctree-l1"><a class="reference internal" href="New_Project.html">Creating a New Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="Robot_Configuration.html">Creating a Robot Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="Path_Planning.html">Path Planning (“World View”)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Robot_Jogging.html">Robot Jogging</a></li>
<li class="toctree-l1"><a class="reference internal" href="File_Browser.html">ROSVITA File Browser</a></li>
<li class="toctree-l1"><a class="reference internal" href="Code_Editor.html">ROSVITA Code Editor</a></li>
<li class="toctree-l1"><a class="reference internal" href="Own_Robot_Models.html">Creation of Own Robot Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Graph_Concept.html">ROSVITA Graph Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="Module_Debugging.html">Module Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lua_Scripts.html">Using Lua Scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="Combination_with_External_ROS.html">Access to ROSVITA from External ROS</a></li>
<li class="toctree-l1"><a class="reference internal" href="Complex_Scene.html">Building a Complex Scene</a></li>
<li class="toctree-l1"><a class="reference internal" href="Python_In_Rosvita_Graphs.html">Python in ROSVITA Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="ROSVITA_and_roslaunch.html">ROSVITA and roslauch</a></li>
<li class="toctree-l1"><a class="reference internal" href="How_To_Stereo_Laser_Line_Client.html">How to use Stereo Laser Line Client</a></li>
<li class="toctree-l1"><a class="reference internal" href="Calibration_Part1.html">Calibration Part1 (Camera, Hand-Eye, Endeffector)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Calibration Part2 (Robot Kinematic)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#capturing-calibration-data-via-automatic-sphere-sampling">Capturing calibration data via automatic sphere sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#further-preparation-of-the-calibration-input-data">Further preparation of the calibration input data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robot-kinematic-calibration">Robot kinematic calibration</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Force_Torque_Data.html">Force Torque Data</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Rosvita documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Calibration Part2 (Robot Kinematic)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/source/Calibration_Part2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="calibration-part2-robot-kinematic">
<span id="calibration-part2-label"></span><h1>Calibration Part2 (Robot Kinematic)<a class="headerlink" href="#calibration-part2-robot-kinematic" title="Permalink to this headline">¶</a></h1>
<p>The Rosvita package <strong>egomo_calibration</strong> (<code class="docutils literal notranslate"><span class="pre">/home/xamla/git/egomo_calibration</span></code>)
containts scripts for robot calibration and hand-eye optimization.
It is written in Python and C++ and uses <a href="http://ceres-solver.org/" target="_blank">Ceres</a> to optimize the robot kinematic
modelled by <a href="https://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters" target="_blank">Denavit-Hartenberg parameters</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The robot kinematic calibration described here, has been used to calibrate an SDA10 robot. For usage with an UR5 or other robots, some adaptions to the scripts will be necessary.</p>
</div>
<p><strong>Calibration pattern requirements:</strong></p>
<p>For all camera and hand-eye calibrations one of our circle patterns with IDs
(<code class="docutils literal notranslate"><span class="pre">/home/xamla/Rosvita.Control/lua/auto_calibration/Patterns_with_ID.pdf</span></code>) has to be used.
See <a class="reference internal" href="Calibration_Part1.html#calibration-patterns-label"><span class="std std-ref">Figure 15.1  Circle patterns with IDs for calibration.</span></a></p>
<div class="section" id="capturing-calibration-data-via-automatic-sphere-sampling">
<span id="sphere-sampling-label"></span><h2>Capturing calibration data via automatic sphere sampling<a class="headerlink" href="#capturing-calibration-data-via-automatic-sphere-sampling" title="Permalink to this headline">¶</a></h2>
<p>Automatic sphere sampling is performed via the <strong>auto_calibration</strong> package of Rosvita (see <a class="reference internal" href="Calibration_Part1.html#calibration-part1-label"><span class="std std-ref">Calibration Part1</span></a>).</p>
<p>To use the automatic sphere sampling, you first have to define a good <strong>starting pose</strong> for the robot arm that will be calibrated. In the following, let us assume a stereo camera setup with the cameras mounted at the endeffector of the robot arm and looking in the direction of the z-axis of the endeffector. With this setup the calibration target has to be fixed onto the table and the robot has to be moved to a pose where the cameras look down at the target approximately straight from above and such that all target points are in the field of view (FOV) of the cameras (see Fig. 16.1). Save this starting pose of the robot (or better the joint values) to the Rosvita world view and move the robot to this posture before starting the sphere sampling.</p>
<div class="figure align-center" id="id1">
<img alt="../_images/SphereSampling_Start.png" src="../_images/SphereSampling_Start.png" />
<p class="caption"><span class="caption-text">Figure 16.1  Starting pose for sphere sampling with left arm.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>Moveover, an initial guess hand-eye and stereo camera calibration is needed, which can be received e.g. by running the <strong>auto_calibration</strong> scripts <strong>configureCalibration.lua</strong> and <strong>runCalibration.lua</strong> (see <a class="reference internal" href="Calibration_Part1.html#calibration-part1-label"><span class="std std-ref">Calibration Part1</span></a>).</p>
<p>Now, to start the sphere sampling, with the Rosvita terminal go into your project folder and run the calibration script from the <strong>auto_calibration</strong> package:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /home/xamla/Rosvita.Control/projects/&lt;your_project_folder&gt;
th ../../lua/auto_calibration/runCalibration.lua -cfg &lt;name_of_your_configuration_file&gt;.t7
</pre></div>
</div>
<p>Here, the configuration file is the one you created before to get an initial hand-eye and stereo camera calibration (see previous chapter).</p>
<p>Then press</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>d <span class="o">(</span>Capture sphere sampling<span class="o">)</span>
</pre></div>
</div>
<p>You’ll have to choose the camera setup. Currently, two possibilities are implemented:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>e end-of-arm camera setup
t torso camera setup
</pre></div>
</div>
<p>Choose the option e (end-of-arm camera setup).</p>
<p>After that, you have to enter the number of capture poses you want to sample.
In order to obtain good calibration results, you should choose a rather large number of about 100-200 poses.</p>
<p>Next you have to enter the paths to the previously generated intial guesses for the hand-eye calibration and stereo camera calibration (e.g. <code class="docutils literal notranslate"><span class="pre">&lt;path-to-your-project-folder&gt;/calibration/&lt;date&gt;_&lt;time&gt;/HandEye.t7</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;path-to-your-project-folder&gt;/calibration/&lt;date&gt;_&lt;time&gt;/stereo_cams_&lt;serial1&gt;_&lt;serial2&gt;.t7</span></code>). Then after accepting the <strong>identified target point</strong> by pressing <strong>enter</strong> the sphere sampling will begin, i.e. the robot will start moving and recording images and poses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In rare cases it may happen that the <strong>identified target point</strong> contains unrealistic large values. In such cases, the planner will not find a valid joint configuration, which will result in an endless loop with “nan” outputs. If you notice such a case, simply interrupt the current capturing process by pressing <code class="docutils literal notranslate"><span class="pre">Ctrl+c</span></code>, restart the calibration script and repeat the steps from before.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Make sure, that all <strong>collision objects</strong> in your robot’s work space are <strong>modeled carefully</strong> (with safety margin), before starting the sphere sampling. The <strong>robot will move relatively fast</strong> using MoveIt! and collision check. However, collisions can only be avoided for correctly modeled collision objects.</p>
</div>
</div>
<div class="section" id="further-preparation-of-the-calibration-input-data">
<span id="further-preparation-label"></span><h2>Further preparation of the calibration input data<a class="headerlink" href="#further-preparation-of-the-calibration-input-data" title="Permalink to this headline">¶</a></h2>
<p><strong>Sphere sampling output folder structure</strong></p>
<p>Now, you have to prepare your data obtained from the sphere sampling for the robot kinematic calibration task.</p>
<p>After the sphere sampling is finished the data lies in the folder <code class="docutils literal notranslate"><span class="pre">/tmp/calibration/capture_sphere_sampling/</span></code>.
This folder contains the following files:</p>
<ul class="simple">
<li><p>The 100-200 captured images of the calibration target for camera 1 and 2 (cam_&lt;serial1&gt;_001.png, …, cam_&lt;serial1&gt;_200.png, cam_&lt;serial2&gt;_001.png, …, cam_&lt;serial2&gt;_200.png)</p></li>
<li><p>The robot poses and joint configurations of the relevant move group (jsposes.t7, jsposes_tensors.t7)</p></li>
<li><p>The starting pose and joint configuration of the complete robot (all_vals.t7, all_vals_tensors.t7)
(This is only needed to obtain the static torso position, if the torso is not moved, i.e. does not belong to the relevant move group.)</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <strong>/tmp</strong> location is a <strong>temporary</strong> one. If you want to save your sphere sampling data permanently, you have to move it e.g. into your project folder!</p>
</div>
<p><strong>Improvement of stereo camera and hand-eye input data</strong></p>
<p>Now, with the 200 sampled images and robot poses, you first should determine an improved stereo calibration, as well as an improved initial hand-eye matrix. Thereto, simply copy the captured images into a folder <code class="docutils literal notranslate"><span class="pre">/tmp/calibration/capture/</span></code> and run the camera and hand-eye calibration of the package <strong>auto_calibration</strong> (see <a class="reference internal" href="Calibration_Part1.html#calibration-part1-label"><span class="std std-ref">Calibration Part1</span></a>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /tmp/calibration/<span class="p">;</span> mkdir capture
cp -r capture_sphere_sampling/*.png capture/
<span class="nb">cd</span> /home/xamla/Rosvita.Control/projects/&lt;your-project-folder&gt;/
th ../../lua/auto_calibration/runCalibration.lua -cfg &lt;your_configuration_file&gt;.t7
a <span class="o">(</span>Calibrate camera<span class="o">)</span>
s <span class="o">(</span>Save calibration<span class="o">)</span>
b <span class="o">(</span>Hand-eye calibration<span class="o">)</span>
</pre></div>
</div>
<p>When you have to enter the name of the folder containing the <strong>jsposes.t7</strong> file, type
<code class="docutils literal notranslate"><span class="pre">capture_sphere_sampling</span></code>.</p>
<p>Finally, move the results of this calibration into the sphere sampling output folder:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mv /tmp/calibration/&lt;date&gt;_&lt;time&gt;/stereo_cams_&lt;serial1&gt;_&lt;serial2&gt;.t7 /tmp/calibration/capture_sphere_sampling/
mv /tmp/calibration/&lt;date&gt;_&lt;time&gt;/HandEye.t7 /tmp/calibration/capture_sphere_sampling/
</pre></div>
</div>
<p><strong>Data conversion</strong></p>
<p>The egomo_calibration algorighm is written in Python and needs numpy arrays (.npy files) as input files.
Thus, you have to convert the lua .t7 files into the .npy format.
To do this, use the script <code class="docutils literal notranslate"><span class="pre">/home/xamla/git/egomo_calibration/examples/run_data_conversion.sh</span></code>,
i.e. adapt the camera serials within this script, then go into your data folder (<code class="docutils literal notranslate"><span class="pre">capture_sphere_sampling</span></code>)
and call the script from there:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /tmp/calibration/capture_sphere_sampling/
/home/xamla/git/egomo_calibration/examples/run_data_conversion.sh
</pre></div>
</div>
</div>
<div class="section" id="robot-kinematic-calibration">
<span id="robot-kinematic-calibration-label"></span><h2>Robot kinematic calibration<a class="headerlink" href="#robot-kinematic-calibration" title="Permalink to this headline">¶</a></h2>
<p>Now, you can run the robot kinematic calibration with the previously captured and prepared input data.
Thereto, first adapt the corresponding start script
(<code class="docutils literal notranslate"><span class="pre">/home/xamla/git/egomo_calibration/examples/run_dh_calib_motoman_end_of_arm_cameras.sh</span></code> or
<code class="docutils literal notranslate"><span class="pre">/home/xamla/git/egomo_calibration/examples/run_dh_calib_motoman_end_of_arm_cameras_v2.sh</span></code>), i.e.
you have to adapt the paths to your input data, the number of captured images, the ID of the used circle pattern,
the output file names, the parameters you want to optimize, etc.
A detailed list of these input arguments is given at the beginning of the start script.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The argument <strong>alternating optimization</strong> means that DH-parameters and hand-eye are repeatedly optimized after each other. Thus this argument should only be <strong>True</strong>, if <strong>optimize hand-eye</strong> is set to <strong>True</strong>. Moreover, <strong>with_torso_optimization</strong> should only be set to <strong>True</strong>, if <strong>with_torso_movement_in_data</strong> is also <strong>True</strong>, because if there is no torso movement within the data, the torso joint cannot be optimized.</p>
</div>
<p>Next, with the terminal go into the folder containing the start script and call the script from there:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /home/xamla/git/egomo_calibration/examples/
./run_dh_calib_motoman_end_of_arm_cameras.sh
</pre></div>
</div>
<p>or:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_dh_calib_motoman_end_of_arm_cameras_v2.sh
</pre></div>
</div>
<p>The <strong>first variant</strong> uses an average of the 3d circle pattern as initial guess. In more detail, for each stereo image pair the 3-dimensional pattern points in camera coordinates are calculated by triangulation and transformed into base coordinates by multiplication with the robot pose and hand-eye matrix. Then each 3d circle point position is averaged for all ~200 captured image pairs and the resulting average circle point pattern is taken as ground truth for calculating the reprojection error. In the reprojection error calculation each observed 2d pattern point is compared to the corresponding ground truth pattern point, which is the previously calculated average 3d pattern point projected back into 2d by using the current hand-eye and robot kinematic. (Note, that by setting <strong>optimize points</strong> to <strong>True</strong>, the averaged pattern points will also be optimized.)</p>
<p>The <strong>second variant (v2)</strong> calculates the reprojection error by comparing each circle pattern point with each other circle pattern point at the same position in the pattern for all ~200 images. Pattern points are in 3d and transformed into base coordinates with help of the current hand-eye and robot kinematic. This second variant should be <strong>more precise</strong>, but also takes <strong>more time</strong>.</p>
<p>As result, the program writes the optimized robot model and hand-eye into .npy files. Moreover, an urdf with the optimized values is written into <strong>calibration_result.urdf</strong>. <strong>Copy</strong> this <strong>urdf into your current project</strong> then close and <strong>reopen</strong> your <strong>project</strong> to apply the optimized values of the new urdf. Moreover, <strong>publish</strong> the <strong>optimized hand-eye</strong> into the Rosvita World View, i.e. with the Rosvita terminal go into your project folder and run the following <strong>publish_hand_eye.py</strong> script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /home/xamla/Rosvita.Control/projects/&lt;your_project_folder&gt;
python3 /home/xamla/git/egomo_calibration/examples/publish_hand_eye.py
</pre></div>
</div>
<p>You will have to enter the name (with path) of the optimized hand-eye and to choose the corresponding robot arm.
As a result, the optimized hand-eye will be published into the Rosvita World View folder “Calibration” (create this folder previously, if not already existing).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you use the first variant together with an alternating optimization of DH-parameters and hand-eye,
you should not choose more than 4 runs, because the precision of the first variant seems to be limited and
for more than 4 runs, the optimization process probably will diverge.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you use the second variant (v2) with torso joint optimization, the result for the optimal torso joint “theta[0]” is to be regarded with scepticism.
In contrast to all other optimized angles and lengths, which show reasonably small optimal changes,
the optimal torso joint offset often seems to be quite large.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Force_Torque_Data.html" class="btn btn-neutral float-right" title="Force Torque Data" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Calibration_Part1.html" class="btn btn-neutral float-left" title="Calibration Part1 (Camera, Hand-Eye, Endeffector)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Xamla.com
      <span class="lastupdated">
<<<<<<< HEAD
        Last updated on May 29, 2019.
=======
        Last updated on Jun 03, 2019.
>>>>>>> 3a45550... add roslaunch in ROSVITA documentation
      </span>

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>